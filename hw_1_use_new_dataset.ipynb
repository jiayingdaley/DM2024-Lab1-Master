{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: 吳佳穎\n",
    "\n",
    "Student ID: 11375007\n",
    "\n",
    "GitHub ID: jiayingdaley"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: do the take home exercises in the DM2024-Lab1-Master. You may need to copy some cells from the Lab notebook to this notebook. This part is worth 20% of your grade.\n",
    "2. Second: follow the same process from the DM2024-Lab1-Master on the new dataset. You don't need to explain all details as we did (some minimal comments explaining your code are useful though). This part is worth 30% of your grade.\n",
    "Download the the new dataset. The dataset contains a sentiment and comment columns, with the sentiment labels being: 'nostalgia' and 'not nostalgia'\n",
    ". Read the specificiations of the dataset for background details.\n",
    "You are allowed to use and modify the helper functions in the folder of the first lab session (notice they may need modification) or create your own.\n",
    "3. Third: please attempt the following tasks on the new dataset. This part is worth 30% of your grade.\n",
    "Generate meaningful new data visualizations. Refer to online resources and the Data Mining textbook for inspiration and ideas.\n",
    "Generate TF-IDF features from the tokens of each text. This will generating a document matrix, however, the weights will be computed differently (using the TF-IDF value of each word per document as opposed to the word frequency). Refer to\n",
    "this Scikit-learn guide .\n",
    "Implement a simple Naive Bayes classifier that automatically classifies the records into their categories. Use both the TF-IDF features and word frequency features to build two seperate classifiers. Note that for the TF-IDF features you might\n",
    "need to use other type of NB classifier different than the one in the Master Notebook. Comment on the differences. Refer to this article.\n",
    "4. Fourth: In the lab, we applied each step really quickly just to illustrate how to work with your dataset. There are somethings that are not ideal or the most efficient/meaningful. Each dataset can be handled differently as well. What are those inefficent\n",
    "parts you noticed? How can you improve the Data preprocessing for these specific datasets? This part is worth 10% of your grade.\n",
    "5. Fifth: It's hard for us to follow if your code is messy, so please tidy up your notebook and add minimal comments where needed. This part is worth 10% of your grade.\n",
    "\n",
    "You can submit your homework following these guidelines: Git Intro & How to hand your homework. Make sure to commit and save your changes to your repository BEFORE the deadline (October 27th 11:59 pm, Sunday).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/DataMining/lib/python3.9/site-packages (2024.10.0)\n",
      "Requirement already satisfied: huggingface_hub in /opt/anaconda3/envs/DataMining/lib/python3.9/site-packages (0.26.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/DataMining/lib/python3.9/site-packages (from huggingface_hub) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/DataMining/lib/python3.9/site-packages (from huggingface_hub) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/envs/DataMining/lib/python3.9/site-packages (from huggingface_hub) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/DataMining/lib/python3.9/site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/DataMining/lib/python3.9/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/anaconda3/envs/DataMining/lib/python3.9/site-packages (from huggingface_hub) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/DataMining/lib/python3.9/site-packages (from huggingface_hub) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/DataMining/lib/python3.9/site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/DataMining/lib/python3.9/site-packages (from requests->huggingface_hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/DataMining/lib/python3.9/site-packages (from requests->huggingface_hub) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/DataMining/lib/python3.9/site-packages (from requests->huggingface_hub) (2024.8.30)\n",
      "準確率： 0.8422222222222222\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    nostalgia       0.79      0.93      0.85       225\n",
      "not nostalgia       0.91      0.76      0.83       225\n",
      "\n",
      "     accuracy                           0.84       450\n",
      "    macro avg       0.85      0.84      0.84       450\n",
      " weighted avg       0.85      0.84      0.84       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# 載入資料集\n",
    "!pip install fsspec\n",
    "!pip install huggingface_hub\n",
    "\n",
    "df = pd.read_csv(\"hf://datasets/Senem/Nostalgic_Sentiment_Analysis_of_YouTube_Comments_Data/Nostalgic_Sentiment_Analysis_of_YouTube_Comments_Data.csv\")\n",
    "\n",
    "# 資料集是一個 CSV 檔案，包含 'comment' 和 'sentiment' 兩欄。\n",
    "data = pd.read_csv(\"Nostalgic_Sentiment_Analysis_of_YouTube_Comments_Data.csv\") \n",
    "\n",
    "# 清理資料（範例：移除標點符號）\n",
    "data['comment'] = data['comment'].str.replace('[^\\w\\s]', '') \n",
    "\n",
    "# 建立詞彙表和文件詞條矩陣\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(data['comment'])\n",
    "\n",
    "# 分割資料集為訓練集和測試集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, data['sentiment'], test_size=0.3, random_state=42)\n",
    "\n",
    "# 使用多項式樸素貝葉斯分類器進行訓練\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 進行預測\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# 評估模型效能\n",
    "print(\"準確率：\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
